{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoauL02lV4gSvBIBqsauFI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santosw-Git/Transformer_code_from_scratch/blob/main/Transformer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. **Import Modules**"
      ],
      "metadata": {
        "id": "tW1ImgzQXcuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "qmBOlIrCXVHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Input Embedding**"
      ],
      "metadata": {
        "id": "tUne3JYqXxYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self,d_model:int,vocab_size:int):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.vocab_size=vocab_size\n",
        "    self.embedding=nn.Embedding(vocab_size,d_model)\n",
        "\n",
        "  def forword(self,x):\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)\n"
      ],
      "metadata": {
        "id": "HM9mGx3GX2Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.**Positional_Encoding**"
      ],
      "metadata": {
        "id": "iBxDSGl1chTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model:int,seq_len:int,dropout:float):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.seq_len=seq_len\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    PE=torch.zeros(self.seq_len,self.d_model)\n",
        "    position=torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1)\n",
        "    div_term=torch.exp(torch.arange(0,self.d_model,2).float() * (-math.log(10000.0)/self.d_model))\n",
        "    PE[:,0::2]=torch.sin(position * div_term)\n",
        "    PE[:,1::2]=torch.cos(position * div_term)\n",
        "    PE=PE.unsqueeze(0)\n",
        "    self.register_buffer(\"PE\",PE)\n",
        "\n",
        "  def forword(self,x):\n",
        "    x=x+ (self.PE[:,:x.shape[1],:]).requires_grad(False)\n",
        "    return self.dropout(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uaFbZF-Lcm0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.MultiHead **Attention**"
      ],
      "metadata": {
        "id": "hZVXIz8se-Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model:int,head:int,dropout:float):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.head=head\n",
        "    assert self.d_model % self.head==0, \"d_model is not divisible by head\"\n",
        "    self.d_k=d_model//head\n",
        "    self.w_q=nn.Linear(d_model,d_model)\n",
        "    self.w_k=nn.Linear(d_model,d_model)\n",
        "    self.w_v=nn.Linear(d_model,d_model)\n",
        "    self.w_o=nn.Linear(d_model,d_model)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query,key,value,mask,dropout:nn.Dropout):\n",
        "    d_k=query.shape[-1]\n",
        "    attention_score=(query @ key.transpose(-2,-1))/math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "      attention_score.masked_fill_(mask==0 , -1e9)\n",
        "    attention_score=attention_score.softmax(dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_score=dropout(attention_score)\n",
        "    return (attention_score @ value) , attention_score\n",
        "\n",
        "\n",
        "  def forward(self,q,k,v,mask):\n",
        "    query=self.w_q(q) #(1,11,512)\n",
        "    key=self.w_k(k)\n",
        "    value=self.w_v(v)\n",
        "    #from here it is for mutihead and above it was for self attention\n",
        "    query=query.view(query.shape[0],query.shape[1],self.head,self.d_k).transpose(1,2) #[1,8,11,64]\n",
        "    key=key.view(key.shape[0],key.shape[1],self.head,self.d_k).transpose(1,2)\n",
        "    value=value.view(value.shape[0],value.shape[1],self.head,self.d_k).transpose(1,2)\n",
        "\n",
        "    x,self.attention_score=MultiHeadAttention.attention(query,key,value,mask,self.dropout)\n",
        "    x=x.transpose(1,2).contiguous().view(x.shape[0],-1,self.head * self.d_k)\n",
        "    return self.w_o(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "jQTjhbsB9en8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Layer **Normalization**"
      ],
      "metadata": {
        "id": "fbxFCtIDlQKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self,eps:float=10**-6) -> None:\n",
        "    super().__init__()\n",
        "    self.eps=eps\n",
        "    self.alpha=nn.Parameter(torch.ones(1))\n",
        "    self.bias=nn.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean=x.mean(dim=-1,keepdim=True)\n",
        "    std=x.std(dim=-1,keepdim=True)\n",
        "    return self.alpa * (x-mean)/(std+self.eps) + self.bias\n",
        "\n"
      ],
      "metadata": {
        "id": "4BG3gf5TpXb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Feed Forward **Network**"
      ],
      "metadata": {
        "id": "uHxV2obIcoON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "  def __init__(self,d_model:int,d_ff:int,dropout:float)-> None:\n",
        "    super.__init__()\n",
        "    self.d_model=d_model\n",
        "    self.linear_1=nn.Linear(self.d_model,d_ff)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.linear_2=nn.Linear(d_ff,self.d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lAGbi9Adcne6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.Residual **Connection**"
      ],
      "metadata": {
        "id": "0nQeqmNwkRHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self,dropout:float) -> None:\n",
        "    super.__init__()\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.norm=LayerNormalization()\n",
        "\n",
        "  def forward(self,x,layer):\n",
        "    # return x + self.dropout(self.norm(layer(x)))\n",
        "    return x + self.dropout(layer(self.norm(x)))\n"
      ],
      "metadata": {
        "id": "BmIBYud8kVcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7.Encoder**"
      ],
      "metadata": {
        "id": "5mzkAtGRmC_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self,self_attention_block:MultiHeadAttention , feed_forward_block:FeedForwardNetwork,dropout:float)->None:\n",
        "    super.__init__()\n",
        "    self.self_attention_block=self_attention_block\n",
        "    self.feed_forward_block=feed_forward_block\n",
        "    self.residual_connections=nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self,x,src_mask):\n",
        "    x=self.residual_connections[0](x,lambda x : self.self_attention_block(x,x,x,src_mask))\n",
        "    x=self.residual_connections[1](x,self.feed_forward_block)\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HdyJx-rlmGR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,layers:nn.ModuleList)->None:\n",
        "    super.__init__()\n",
        "    self.layers=layers\n",
        "    self.norm=LayerNormalization()\n",
        "\n",
        "  def forward(self,x,mask):\n",
        "    for layer in self.layers:\n",
        "      x=layer(x,mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BbgENCa9o4wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.Decoder **Block**"
      ],
      "metadata": {
        "id": "vVBBh9JGdAQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self,self_attention_block:MultiHeadAttentionBlock,cross_attention_block:MultiHeadAttentionBlock,feed_forward_block:FeedForwardNetwork,dropout:float)->None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block=self_attention_block\n",
        "    self.cross_attention_block=cross_attention_block\n",
        "    self.feed_forward_block=feed_forward_block\n",
        "    self.residual_connections=nn.ModuleList([ResidualConection for _ in range(3)])\n",
        "\n",
        "  def forward(self,x,encoder_output,src_mask,target_mask):\n",
        "    x=self.residual_connections[0](x,lambda x: self.self_attention_block(x,x,x,target_mask))\n",
        "    x=self.residual_connections[1](x,lambda x:self.cross_attention_block(d,encoder_output,encoder_output,src_mask))\n",
        "    x=self.residual_connections[2](s,feed_forward_block)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "dWyHSXc0dEYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,layers:nn.ModuleList)->None:\n",
        "    super().__init__()\n",
        "    self.layers=layers\n",
        "    self.norm=LayerNormalization()\n",
        "\n",
        "  def forward(self,x,encoder_output,src_mask,target_mask):\n",
        "    for layer in self.layers:\n",
        "      x=layer(x,encoder_output,src_mask,target_mask)\n",
        "    return self.norm(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "p9lXaMrrhGCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.**Linear_layer**"
      ],
      "metadata": {
        "id": "dYtq-Akjhyfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "  def __init__(self,d_model:int,vocab_size:int) -> None:\n",
        "    super().__init__()\n",
        "    self.proj=nn.Linear(d_model,vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return torch.log_softmax(self.proj(x),dim=-1)"
      ],
      "metadata": {
        "id": "T2mkO24fhxfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.Building the Transformer"
      ],
      "metadata": {
        "id": "QZpUx1fL8rV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,encoder:Encoder,decoder:Decoder,src_embed:InputEmbeddings,target_embed:InputEmbeddings,src_pos:PositionalEmbedding,target_pos:Position_Embedding,projection_layer:ProjectionLayer):\n",
        "    super().__init__()\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.src_embed=src_embed\n",
        "    self.target_embed=target_embed\n",
        "    self.src_pos=src_pos\n",
        "    self.target_pos=target_pos\n",
        "    self.projection_layer=projection_layer\n",
        "\n",
        "  def encoder(self,src,src_mask):\n",
        "    src=self.src_embed(src)\n",
        "    src=self.src_pos(src)\n",
        "    return self.encoder(src,src_mask)\n",
        "\n",
        "  def decoder(self,encoder_output,src_mask,target,target_mask):\n",
        "    target=self.target_embed(target)\n",
        "    target=self.target_pos(target)\n",
        "    return self.decoder(target,encoder_output,src_mask,target_mask)\n",
        "\n",
        "  def project(self,x):\n",
        "    return self.projection_layer(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MGvMXmDx8wL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size:int,target_vocab_size:int,src_seq_len:int,target_seq_len:int,d_model:int=512,N:int=6,head:int=8,dropout:float=0.1,d_ff:int=2048) -> Transformer:\n",
        "  src_embed=InputEmbeddings(d_model,src_vocab_size)\n",
        "  target_embded=PositionalEmbedding(d_model,target_vocab_size)\n",
        "  src_pos=PositionalEncoding(d_model,src_seq_len,dropout)\n",
        "  target_pos=PositionalEncoding(d_model,target_seq_len)\n",
        "  encoder_blocks=[]\n",
        "\n",
        "  for _ in range(N):\n",
        "    encoder_self_attention_block=MultiHeadAttention(d_model,head,dropout)\n",
        "    feed_forward_block=FeedForwardNetwork(d_model,d_ff,dropout)\n",
        "    encoder_block=EncoderBlock(encoder_self_attention_block,feed_forward_block)\n",
        "    encoder_blocks.append(encoder_block)\n",
        "\n",
        "  decoder_blocks=[]\n",
        "  for _ in range(N):\n",
        "    decoder_self_attention_block=MultiHeadAttention(d_model,head,dropout)\n",
        "    decoder_cross_attention_block=MultiHeadAttention(d_model,head,dropout)\n",
        "    feed_forward_block=FeedForwardNetwork(d_model,d_ff,dropout)\n",
        "    decoder_block=DecoderBlock(decoder_self_attention_block,decoder_cross_attention_block,feed_forward_block,dropout)\n",
        "    decoder_blocks.append(decoder_block)\n",
        "\n",
        "  encoder=Encoder(nn.ModuleList(encoder_blocks))\n",
        "  decoder=Decoder(nn.ModuleList(decoder_blocks))\n",
        "  projection=ProjectionLayer(d_model,target_vocab_size)\n",
        "  transformer=Transformer(encoder,decoder,src_embed,target_embed,src_pos,target_pos,projection)\n",
        "\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim()>1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "PrDqEpXL_hoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11.Tokenizer**"
      ],
      "metadata": {
        "id": "SLh0uF6tmz38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from pathlib import Path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoHWh6YCngtB",
        "outputId": "14944407-6778-42c3-a110-18c9875e42ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_or_build_tokenizer(config,ds,lang):\n",
        "  tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))"
      ],
      "metadata": {
        "id": "d8YIQsrRvjU3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}